#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bossç›´è˜AI Agentå²—ä½çˆ¬è™« - Playwrightç‰ˆæœ¬
logger.
"""

import json
import asyncio
import random
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import quote, urlparse, parse_qs
from config import SiteConfig
from local_type import JobDetailItem, JobDetailResponse, JobListItem, JobListResponse, UserInput, JobItemOrDetailItem
from playwright.async_api import async_playwright, Page, Playwright, Browser, Route
from playwright.async_api import BrowserContext as Context
from playwright_stealth import Stealth
import logging
from util.fs import exists_file, write_json, delete_file, read_json
from util.common import filter_job_list, get_unique_job_list, get_unique_job_details
from tqdm import tqdm
import time
import questionary

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BossSpider:
    def __init__(self, site_config: SiteConfig):
        self.playwright: Playwright | None = None
        self.browser: Browser | None = None
        self.context: Context | None = None
        self.page: Page | None = None
        self.site_config: SiteConfig = site_config
        self.is_login: bool = False
        self.current_page: int = 1
        self.job_list: list[JobListItem] = []
        self.job_details: list[JobDetailItem] = []

    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        if self.playwright:
            return

        custom_languages = ('zh-CN', 'en')
        stealth = Stealth(
            navigator_languages_override=custom_languages,
            init_scripts_only=True
        )

        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=False)
        if not self.browser:
            raise Exception("æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")

        self.context = await self.browser.new_context(
            storage_state=self.site_config.auth_path if exists_file(self.site_config.auth_path) else None)
        if not self.context:
            raise Exception("ä¸Šä¸‹æ–‡åˆå§‹åŒ–å¤±è´¥")

        await stealth.apply_stealth_async(self.context)

        self.page = await self.context.new_page()
        logger.info("æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆ, æ‰“å¼€äº†æ–°é¡µé¢")
        if not self.page:
            raise Exception("é¡µé¢åˆå§‹åŒ–å¤±è´¥")

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            await self.page.close()
            self.page = None
        if self.context:
            await self.context.close()
            self.context = None
        if self.browser:
            await self.browser.close()
            self.browser = None
        if self.playwright:
            await self.playwright.stop()
            self.playwright = None
        logger.info("æµè§ˆå™¨å…³é—­å®Œæˆ")

    async def save_auth(self):
        """ä¿å­˜è®¤è¯ä¿¡æ¯"""
        if not self.context:
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        if self.is_login:
            logger.info("ä¿å­˜è®¤è¯ä¿¡æ¯")
            await self.context.storage_state(path=self.site_config.auth_path)
        else:
            logger.info("åˆ é™¤è®¤è¯ä¿¡æ¯")
            delete_file(self.site_config.auth_path)

    async def detect_login_status(self, need_goto: bool = True):
        """æ£€æµ‹ç™»å½•çŠ¶æ€"""
        if not self.page:
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        logger.info("å¼€å§‹æ£€æµ‹ç™»å½•çŠ¶æ€")

        try:
            if need_goto:
                await self.page.goto(self.site_config.urls.home_page_url)
            user_name = await self.page.locator('[ka=header-username]').all()
            self.is_login = len(user_name) > 0
            logger.info(f"ç™»å½•çŠ¶æ€: {self.is_login}")
        except Exception as e:
            logger.error(f"æ£€æµ‹ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            self.is_login = False

    def has_login(self):
        """æ˜¯å¦å·²ç™»å½•"""
        return self.is_login

    async def handle_joblist_response(self, route: Route, job_list: list[JobListItem]):
        """å¤„ç†å²—ä½åˆ—è¡¨å“åº”"""
        logger.info(f"å¤„ç†å²—ä½åˆ—è¡¨å“åº”: {route.request.url}")

        qs = parse_qs(urlparse(route.request.url).query)
        next_page = int(qs.get('page', ['1'])[0])
        self.page_size = int(qs.get('pageSize', ['10'])[0])
        if self.current_page != next_page:
            self.current_page = next_page

        try:
            original = await route.fetch()
            body = await original.body()
            json_data: JobListResponse = json.loads(body.decode('utf-8'))
            if json_data.get('code') == 0:
                job_list.extend(json_data.get('zpData', {}).get('jobList', []))
                write_json(job_list, 'data/joblist.json')

            body = json.dumps(json_data).encode('utf-8')

            await route.fulfill(
                status=original.status,
                headers=original.headers,
                body=body
            )
        except Exception as e:
            logger.error(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ç»§ç»­è¯·æ±‚
            await route.continue_()

    async def handle_detail_response(self, route: Route, job_details: list[JobDetailItem]):
        """å¤„ç†å²—ä½è¯¦æƒ…å“åº”"""
        try:
            logger.info(f"å¤„ç†å²—ä½è¯¦æƒ…å“åº”: {route.request.url}")
            original = await route.fetch()
            body = await original.body()
            json_data: JobDetailResponse = json.loads(body.decode('utf-8'))
            if json_data.get('code') == 0:
                job_details.append(json_data.get('zpData', {}))

            body = json.dumps(json_data).encode('utf-8')

            await route.fulfill(
                status=original.status,
                headers=original.headers,
                body=body
            )
        except Exception as e:
            logger.error(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ç»§ç»­è¯·æ±‚
            await route.continue_()

    async def scroll_page(self, user_input: UserInput, job_index: int):
        """æ»šåŠ¨é¡µé¢"""
        logger.info(f"å°è¯•æ»šåŠ¨é¡µé¢, ç›®æ ‡å²—ä½æ•°é‡: {user_input['max_size']}")

        if not self.page:
            logger.error("é¡µé¢æœªåˆå§‹åŒ–")
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        last_height = 0
        while len(filter_job_list(self.job_list, user_input)) < user_input['max_size'] * job_index:
            current_height = await self.page.evaluate("document.body.scrollHeight")
            # å¦‚æœé«˜åº¦æ²¡æœ‰å˜åŒ–ï¼Œåˆ™è®¤ä¸ºå·²ç»æ»šåŠ¨åˆ°åº•éƒ¨
            if current_height == last_height:
                logger.warning("é¡µé¢é«˜åº¦æ²¡æœ‰å˜åŒ–ï¼Œè®¤ä¸ºå·²ç»æ»šåŠ¨åˆ°åº•éƒ¨")
                break
            last_height = current_height
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.page.wait_for_load_state('networkidle')
            await asyncio.sleep(random.uniform(1, 2))

        logger.info(
            f"å…±æ£€ç´¢åˆ° {len(filter_job_list(self.job_list, user_input))} ä¸ªå²—ä½")

    async def click_all_jobs(self, filtered_job_list: list[JobListItem]):
        """ç‚¹å‡»æ‰€æœ‰å²—ä½"""
        if not self.page:
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        logger.info("å¼€å§‹ç‚¹å‡»æ‰€æœ‰å²—ä½")

        job_list = await self.page.locator('.card-area .job-name').all()
        if len(job_list) < 2:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å²—ä½")
            return

        encrypt_job_ids = [job['encryptJobId'] for job in filtered_job_list]
        # é¡µé¢é»˜è®¤ä¼šåŠ è½½ç¬¬ä¸€æ¡ï¼Œæ‰€ä»¥å…ˆç‚¹å‡»ç¬¬äºŒæ¡ï¼Œå†ç‚¹å‡»ç¬¬ä¸€æ¡ï¼Œç¡®ä¿èƒ½è§¦å‘è¯¦æƒ…é¡µçš„è¯·æ±‚
        new_job_list = []
        for job in job_list:
            try:
                href = await job.get_attribute('href') or ''
                current_encrypt_job_id = href.split('/')[-1].split('.')[0]
                if current_encrypt_job_id in encrypt_job_ids:
                    new_job_list.append(job)
            except Exception as e:
                logger.error(f"è·å–å²—ä½é“¾æ¥æ—¶å‡ºé”™: {e}")
                continue

        if len(new_job_list) < 2:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å²—ä½")
            return

        new_job_list = [new_job_list[1], new_job_list[0]] + new_job_list[2:]
        for job in tqdm(new_job_list, desc="ç‚¹å‡»å²—ä½è¯¦æƒ… ğŸ”"):
            if self.page.is_closed():
                logger.warning("é¡µé¢å·²å…³é—­, é€€å‡º")
                return
            try:
                await job.click()
                await self.page.wait_for_load_state('load')
                await asyncio.sleep(random.uniform(1, 3))
            except Exception as e:
                logger.error(f"ç‚¹å‡»å²—ä½æ—¶å‡ºé”™: {e}")
                continue

    async def wait_for_url_change(self, initial_url: str, timeout: int = 60):
        """ç­‰å¾…åœ°å€æ å˜åŒ–"""
        start_time = time.time()
        while self.page and not self.page.is_closed():
            if self.page.url != initial_url:
                logger.info(f"åœ°å€æ å˜åŒ–ä¸º {self.page.url}")
                return True
            await asyncio.sleep(random.uniform(1, 3))
            if time.time() - start_time > timeout:  # è¶…æ—¶
                return False
        return False

    async def get_search_keywords(self):
        """è·å–æœç´¢å…³é”®è¯"""
        if not self.page:
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        search_keywords = await self.page.locator('.search-input-box input').input_value()
        return search_keywords.strip()

    async def search_job(self, job_name: str):
        """æœç´¢å²—ä½"""
        if not self.page:
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        page_url = self.page.url
        url_obj = urlparse(page_url)

        logger.info(f"æœç´¢å²—ä½: {job_name}")
        logger.info(f"å½“å‰é¡µé¢: {url_obj.path}")

        input_locators = [
            'input.ipt-search',  # é¦–é¡µ
            '.search-input-box input',  # æœç´¢é¡µ
        ]

        for input_locator in input_locators:
            input_locator = self.page.locator(input_locator)
            if await input_locator.count() > 0:
                await input_locator.fill(job_name)
                await input_locator.press('Enter')
                await self.page.wait_for_load_state('load')
                await asyncio.sleep(random.uniform(2, 4))
                return

        raise Exception(f"æœªæ‰¾åˆ°æœç´¢æ¡†: {job_name}")

    async def search(self, user_input: UserInput):
        """æœç´¢AI Agentå²—ä½"""
        if not self.page:
            raise Exception("é¡µé¢æœªåˆå§‹åŒ–")

        await self.page.route(f'{self.site_config.urls.job_list_url}**', lambda route: self.handle_joblist_response(route, self.job_list))
        await self.page.route(f'{self.site_config.urls.job_detail_url}**', lambda route: self.handle_detail_response(route, self.job_details))

        logger.info("è¯·ç›´æ¥åœ¨æ‰“å¼€çš„é¡µé¢ä¸­æœç´¢ä½ æƒ³è¦çš„å²—ä½ä¿¡æ¯, ç„¶åç‚¹å‡»æœç´¢æŒ‰é’®, å¦‚æœæƒ³é€€å‡º, è¯·ç›´æ¥å…³é—­æµè§ˆå™¨")
        await self.detect_login_status(need_goto=False)
        await self.save_auth()

        for job_index, job_name in enumerate(user_input['job_names'], 1):
            logger.info(f"å¼€å§‹æœç´¢ç¬¬ {job_index} ä¸ªå²—ä½: {job_name}")
            # è·å–èŒä½åˆ—è¡¨
            await self.search_job(job_name)
            await self.scroll_page(user_input, job_index)  # æ»šåŠ¨é¡µé¢
            await asyncio.sleep(random.uniform(2, 4))
            # ç‚¹å‡»æ‰€æœ‰å²—ä½åˆ—è¡¨
            await self.click_all_jobs(filter_job_list(self.job_list, user_input))

        logger.info(
            f"å¼€å§‹è¿‡æ»¤å²—ä½, è¿‡æ»¤å‰: {len(self.job_list)} ä¸ªå²—ä½åˆ—è¡¨, {len(self.job_details)} ä¸ªå²—ä½è¯¦æƒ…")
        filtered_jobs = get_unique_job_list(self.job_list)
        filtered_job_details = get_unique_job_details(self.job_details)

        logger.info(
            f"è¿‡æ»¤å®Œæˆ, å…±æ‰¾åˆ° {len(filtered_job_details)} ä¸ªå²—ä½è¯¦æƒ…, {len(filtered_jobs)} ä¸ªå²—ä½åˆ—è¡¨")
        return filtered_jobs, filtered_job_details

    def save_to_json(self, job_list: list[JobListItem], job_detail: list[JobDetailItem]):
        """ä¿å­˜åˆ°JSON"""
        write_json(job_list, 'data/joblist.json')
        write_json(job_detail, 'data/jobdetail.json')


async def search(user_input: UserInput):
    """ä¸»å‡½æ•°"""
    site_name = 'ZHIPIN'
    site_config = SiteConfig(site_name)
    spider = BossSpider(site_config)
    await spider.init_browser()
    await spider.detect_login_status(need_goto=True)
    if not spider.has_login():
        logger.warning("æœªç™»å½•, æœ€å¤šåªèƒ½æ£€ç´¢ 15 ä¸ªèŒä½, è·³è¿‡ç™»å½•ç»§ç»­æ‰§è¡Œ")
        confirm = await questionary.confirm("å½“å‰æœªç™»å½•, æ˜¯å¦ç»§ç»­æœç´¢(è¯·åœ¨é¡µé¢å®Œæˆç™»å½•ï¼Œç™»å½•åæŒ‰å›è½¦)?", default=True).ask_async()
        if not confirm:
            await spider.close_browser()
            return [], []

    job_list, job_details = await spider.search(user_input=user_input)
    spider.save_to_json(job_list, job_details)
    await spider.close_browser()
    return job_list, job_details


if __name__ == "__main__":
    user_input: UserInput = read_json('data/user_input.json')  # type:ignore
    asyncio.run(search(user_input))
